{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob \n",
    "from skimage.io import imread \n",
    "import shutil\n",
    "%matplotlib inline\n",
    "from random import shuffle\n",
    "import cv2\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import image\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras import layers as KL\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.nasnet import NASNetMobile\n",
    "from keras.applications.xception import Xception\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "!pip install livelossplot\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "from keras.layers import Convolution1D, concatenate, SpatialDropout1D, GlobalMaxPool1D, GlobalAvgPool1D, Embedding, \\\n",
    "    Conv2D, SeparableConv1D, Add, BatchNormalization, Activation, GlobalAveragePooling2D, LeakyReLU, Flatten\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling2D, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, \\\n",
    "    Lambda, Multiply, LSTM, Bidirectional, PReLU, MaxPooling1D,Average\n",
    "from keras.layers.pooling import _GlobalPooling1D\n",
    "from keras.losses import mae, sparse_categorical_crossentropy, binary_crossentropy\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau,CSVLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '.input/train/'\n",
    "TRAIN_LABELS = 'input/train_labels.csv'\n",
    "SIZE_IMG = 96\n",
    "EPOCHS = 2\n",
    "\n",
    "model_path = 'resnet.h5'\n",
    "saved_model = os.path.isfile(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRAIN_LABELS)\n",
    "\n",
    "print(df['label'].value_counts(), \n",
    "      '\\n\\n', df.describe(), \n",
    "      '\\n\\n', df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Init Keras data generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add .tif to ids in the dataframe to use flow_from_dataframe\n",
    "df=pd.read_csv(\"input/train_labels.csv\",dtype=str)\n",
    "def add_ext(id):\n",
    "    return id+\".tif\"\n",
    "\n",
    "df[\"id\"]=df[\"id\"].apply(add_ext)\n",
    "def addpath(col):\n",
    "    return '../input/train/' + col \n",
    "df['Path']=df['id'].apply(addpath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if saved_model:\n",
    "    val = 0\n",
    "else:\n",
    "    val = 0.15\n",
    "    \n",
    "datagen= ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            samplewise_std_normalization= True,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True,\n",
    "            rotation_range=90,\n",
    "            zoom_range=0.2, \n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            shear_range=0.05,\n",
    "            channel_shift_range=0.1,\n",
    "            validation_split=val)\n",
    "\n",
    "train_generator=datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory=TRAIN_PATH,\n",
    "    x_col=\"id\",\n",
    "    y_col=\"label\",\n",
    "    subset=\"training\",\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    class_mode=\"binary\",\n",
    "    target_size=(96,96))\n",
    "\n",
    "valid_generator=datagen.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    directory=TRAIN_PATH,\n",
    "    x_col=\"id\",\n",
    "    y_col=\"label\",\n",
    "    subset=\"validation\",\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    class_mode=\"binary\",\n",
    "    target_size=(96,96))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Build model**\n",
    "The model is a pre-trained Resnet50 with a dense layer combined to a sigmoid activation function for the binary classification. A keras callback was used to reduce the learning rate if the validation accuracy doesn't improve over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_shape = (SIZE_IMG, SIZE_IMG, 3)\n",
    "    inputs = KL.Input(input_shape)\n",
    "    resnet = ResNet50(include_top=False, input_shape=input_shape) \n",
    "    x  = KL.GlobalAveragePooling2D()(resnet(inputs))\n",
    "    x = KL.Dropout(0.5)(x)\n",
    "    outputs = KL.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "def first_training():\n",
    "    '''\n",
    "    train the model and save it if the val_acc test is better than the precedent epoch\n",
    "    '''\n",
    "    model = build_model()\n",
    "    \n",
    "    model.compile(optimizer=Adam(lr=0.0001, decay=0.00001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, \n",
    "                                       verbose=1, mode='max', min_lr=0.000001)\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\"resnet.h5\", monitor='val_acc', verbose=1, \n",
    "                              save_best_only=True, mode='max')\n",
    "\n",
    "    history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=train_generator.n//train_generator.batch_size, \n",
    "                              validation_data=valid_generator,\n",
    "                              validation_steps=valid_generator.n//valid_generator.batch_size,\n",
    "                              epochs=EPOCHS,\n",
    "                              callbacks=[checkpoint,reduce_lr])\n",
    "    \n",
    "    return history, model\n",
    "\n",
    "def second_training():\n",
    "    '''\n",
    "    Tune the model using all available data and a small learning rate\n",
    "    '''\n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    model.compile(optimizer=Adam(lr=0.000001, decay=0.00001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit_generator(train_generator,\n",
    "                              steps_per_epoch=train_generator.n//train_generator.batch_size, \n",
    "                              epochs=10)\n",
    "    \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if saved_model:\n",
    "    history, model = second_training()\n",
    "else:\n",
    "    history, model = first_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_results(epochs):\n",
    "    metrics = ['loss', \"acc\", 'val_loss','val_acc']\n",
    "        \n",
    "    plt.style.use(\"ggplot\")\n",
    "    (fig, ax) = plt.subplots(1, 4, figsize=(30, 5))\n",
    "    fig.subplots_adjust(hspace=0.1, wspace=0.3)\n",
    "\n",
    "    for (i, l) in enumerate(metrics):\n",
    "        title = \"Loss for {}\".format(l) if l != \"loss\" else \"Total loss\"\n",
    "        ax[i].set_title(title)\n",
    "        ax[i].set_xlabel(\"Epoch #\")\n",
    "        ax[i].set_ylabel(l.split('_')[-1])\n",
    "        ax[i].plot(np.arange(0, epochs), history.history[l], label=l)\n",
    "        ax[i].legend() \n",
    "\n",
    "if EPOCHS > 1 and saved_model == False:        \n",
    "    analyse_results(EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'input/test/'\n",
    "df_test = pd.read_csv('input/sample_submission.csv')\n",
    "df_test[\"id\"]=df_test[\"id\"].apply(lambda x : x +\".tif\")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                 samplewise_std_normalization= True)\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=df_test,\n",
    "    directory=test_path,\n",
    "    x_col=\"id\",\n",
    "    y_col=None,\n",
    "    target_size=(96, 96),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=64,\n",
    "    class_mode=None,\n",
    "    shuffle=False,\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "pred=model.predict_generator(test_generator,verbose=1,steps=test_generator.n/2).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test_generator\n",
    "y_preds = model.predict_generator(test_generator,verbose=1,steps=test_generator.n/5)\n",
    "y_pred_keras=y_preds.round()\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(test_labels, y_pred_keras)\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "\n",
    "print('AUC score :', + auc_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CSV submission**\n",
    "Predictions of the test generator are not in the right order so it needs to be rearranged it in the label list before to be passed it to the submission data frame. The final result contains raw predictions without any threshold to classify data because the website is using the ROC curve metric to compute the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict(zip(test_generator.filenames, pred))\n",
    "\n",
    "label = []\n",
    "for i in range(len(df_test[\"id\"])):\n",
    "    label.append(results[df_test[\"id\"][i]])\n",
    "    \n",
    "df_test[\"id\"]=df_test[\"id\"].apply(lambda x : x[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=pd.DataFrame({\"id\":df_test[\"id\"],\n",
    "                      \"label\":label})\n",
    "submission.to_csv(\"resnet_submission.csv\",index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model MobilenetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_from_file_path(file_path):\n",
    "    return file_path.split(os.path.sep)[-1].replace('.tif', '')\n",
    "\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "def data_gen(list_files, id_label_map, batch_size, augment=False):\n",
    "    keras_gen = ImageDataGenerator(\n",
    "                    rotation_range=10,\n",
    "                    width_shift_range=0.1,\n",
    "                    height_shift_range=0.1,\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True,\n",
    "                    zoom_range=0.2,\n",
    "                    shear_range=5)\n",
    "    while True:\n",
    "        shuffle(list_files)\n",
    "        for batch in chunker(list_files, batch_size):\n",
    "            X = [cv2.imread(x) for x in batch]\n",
    "            Y = [id_label_map[get_id_from_file_path(x)] for x in batch]\n",
    "            if augment:\n",
    "                X = [keras_gen.random_transform(x) for x in X]\n",
    "            X = [preprocess_input(x.astype(np.float32)) for x in X]\n",
    "                \n",
    "            yield np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built Model MobilenetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inputs = Input((96, 96, 3))\n",
    "    base_model = MobileNetV2(include_top=False, input_shape=(96, 96, 3))#, weights=None\n",
    "    x = base_model(inputs)\n",
    "    out1 = GlobalMaxPooling2D()(x)\n",
    "    out2 = GlobalAveragePooling2D()(x)\n",
    "    out3 = Flatten()(x)\n",
    "    out = Concatenate(axis=-1)([out1, out2, out3])\n",
    "    out = Dropout(0.5)(out)\n",
    "    out = Dense(1, activation=\"sigmoid\", name=\"3_\")(out)\n",
    "    model = Model(inputs, out)\n",
    "    model.compile(optimizer=Adam(0.0001), loss=binary_crossentropy, metrics=['acc'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"input/train_labels.csv\")\n",
    "id_label_map = {k:v for k,v in zip(df_train.id.values, df_train.label.values)}\n",
    "df_train.head()\n",
    "\n",
    "    \n",
    "labeled_files = glob(r'input/train/*.tif')\n",
    "test_files = glob(r'input/test/*.tif')\n",
    "\n",
    "print(\"labeled_files size :\", len(labeled_files))\n",
    "print(\"test_files size :\", len(test_files))\n",
    "\n",
    "train, val = train_test_split(labeled_files, test_size=0.1, random_state=101010)\n",
    "\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "batch_size=32\n",
    "h5_path = \"mobilenetv2.h5\"\n",
    "checkpoint = ModelCheckpoint(h5_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "history = model.fit_generator(\n",
    "    data_gen(train, id_label_map, batch_size, augment=True),\n",
    "    validation_data=data_gen(val, id_label_map, batch_size),\n",
    "    epochs=2, verbose=1,\n",
    "    callbacks=[checkpoint],\n",
    "    steps_per_epoch=len(train) // batch_size,\n",
    "    validation_steps=len(val) // batch_size)\n",
    "batch_size=64\n",
    "history = model.fit_generator(\n",
    "    data_gen(train, id_label_map, batch_size, augment=True),\n",
    "    validation_data=data_gen(val, id_label_map, batch_size),\n",
    "    epochs=2, verbose=1,\n",
    "    callbacks=[checkpoint],\n",
    "    steps_per_epoch=len(train) // batch_size,\n",
    "    validation_steps=len(val) // batch_size)\n",
    "    \n",
    "model.compile(optimizer=Adam(0.00001), loss=binary_crossentropy, metrics=['acc'])\n",
    "history = model.fit_generator(\n",
    "    data_gen(train, id_label_map, batch_size, augment=True),\n",
    "    validation_data=data_gen(val, id_label_map, batch_size),\n",
    "    epochs=2, verbose=1,\n",
    "    callbacks=[checkpoint],\n",
    "    steps_per_epoch=len(train) // batch_size,\n",
    "    validation_steps=len(val) // batch_size)\n",
    "\n",
    "model.load_weights(h5_path)\n",
    "\n",
    "preds = []\n",
    "ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in chunker(test_files, batch_size):\n",
    "    X = [preprocess_input(cv2.imread(x).astype(np.float32)) for x in batch]\n",
    "    ids_batch = [get_id_from_file_path(x) for x in batch]\n",
    "    X = np.array(X)\n",
    "    preds_batch = ((model.predict(X).ravel()*model.predict(X[:, ::-1, :, :]).ravel()*model.predict(X[:, ::-1, ::-1, :]).ravel()*model.predict(X[:, :, ::-1, :]).ravel())**0.25).tolist()\n",
    "    preds += preds_batch\n",
    "    ids += ids_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id':ids, 'label':preds})\n",
    "df.to_csv(\"mobilenetv2_submission.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Xecption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_LOGS_FILE = \"input/training_logs.csv\"\n",
    "MODEL_SUMMARY_FILE = \"model_summary.txt\"\n",
    "MODEL_FILE = \"xecption.h5\"\n",
    "KAGGLE_SUBMISSION_FILE = \"xception_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'input/'\n",
    "training_dir = input_dir + 'train/'\n",
    "data_frame = pd.DataFrame({'path': glob('input/train/*.tif')})\n",
    "data_frame['id'] = data_frame.path.map(lambda x: x.split('/')[5].split('.')[0])\n",
    "labels = pd.read_csv(input_dir + 'train_labels.csv')\n",
    "data_frame = data_frame.merge(labels, on = 'id')\n",
    "negatives = data_frame[data_frame.label == 0].sample(85000)\n",
    "positives = data_frame[data_frame.label == 1].sample(85000)\n",
    "data_frame = pd.concat([negatives, positives]).reset_index()\n",
    "data_frame = data_frame[['path', 'id', 'label']]\n",
    "data_frame['image'] = data_frame['path'].map(imread)\n",
    "\n",
    "training_path = 'training'\n",
    "validation_path = 'validation'\n",
    "\n",
    "for folder in [training_path, validation_path]:\n",
    "    for subfolder in ['0', '1']:\n",
    "        path = os.path.join(folder, subfolder)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "training, validation = train_test_split(data_frame, train_size=0.9, stratify=data_frame['label'])\n",
    "\n",
    "data_frame.set_index('id', inplace=True)\n",
    "\n",
    "for images_and_path in [(training, training_path), (validation, validation_path)]:\n",
    "    images = images_and_path[0]\n",
    "    path = images_and_path[1]\n",
    "    for image in images['id'].values:\n",
    "        file_name = image + '.tif'\n",
    "        label = str(data_frame.loc[image,'label'])\n",
    "        destination = os.path.join(path, label, file_name)\n",
    "        if not os.path.exists(destination):\n",
    "            source = os.path.join(input_dir + 'train', file_name)\n",
    "            shutil.copyfile(source, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "training_data_generator = ImageDataGenerator(rescale=1./255,\n",
    "                                             horizontal_flip=True,\n",
    "                                             vertical_flip=True,\n",
    "                                             rotation_range=90,\n",
    "                                             zoom_range=0.2, \n",
    "                                             width_shift_range=0.1,\n",
    "                                             height_shift_range=0.1,\n",
    "                                             shear_range=0.05,\n",
    "                                             channel_shift_range=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = training_data_generator.flow_from_directory(training_path,\n",
    "                                                                 target_size=(96,96),\n",
    "                                                                 batch_size=216,\n",
    "                                                                 class_mode='binary')\n",
    "validation_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(validation_path,\n",
    "                                                                              target_size=(96,96),\n",
    "                                                                              batch_size=216,\n",
    "                                                                              class_mode='binary')\n",
    "testing_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(validation_path,\n",
    "                                                                           target_size=(96,96),\n",
    "                                                                           batch_size=216,\n",
    "                                                                           class_mode='binary',\n",
    "                                                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_shape = (96, 96, 3)\n",
    "inputs = Input(in_shape)\n",
    "xception = Xception(include_top = False, weights = None, input_shape = in_shape)  \n",
    "nas_net = NASNetMobile(include_top = False, weights = None, input_shape = in_shape)\n",
    "\n",
    "outputs = Concatenate(axis=-1)([GlobalAveragePooling2D()(xception(inputs)),\n",
    "                                GlobalAveragePooling2D()(nas_net(inputs))])\n",
    "outputs = Dropout(0.5)(outputs)\n",
    "outputs = Dense(1, activation='sigmoid')(outputs)\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer=Adam(lr=0.0001, decay=0.00001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(training_generator,\n",
    "                              steps_per_epoch=len(training_generator), \n",
    "                              validation_data=validation_generator,\n",
    "                              validation_steps=len(validation_generator),\n",
    "                              epochs=10,\n",
    "                              verbose=1,\n",
    "                              callbacks=[PlotLossesKeras(),ModelCheckpoint(MODEL_FILE,\n",
    "                                                                             monitor='val_acc',\n",
    "                                                                             verbose=1,\n",
    "                                                                             save_best_only=True,\n",
    "                                                            mode='max'),CSVLogger(TRAINING_LOGS_FILE,\n",
    "                                                                               append=False,\n",
    "                                                                               separator=';')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_files = glob('input/test/*.tif')\n",
    "submission = pd.DataFrame()\n",
    "for index in range(0, len(testing_files), 5000):\n",
    "    data_frame = pd.DataFrame({'path': testing_files[index:index + 5000]})\n",
    "    data_frame['id'] = data_frame.path.map(lambda x: x.split('/')[5].split(\".\")[0])\n",
    "    data_frame['image'] = data_frame['path'].map(imread)\n",
    "    images = np.stack(data_frame.image, axis=0)\n",
    "    predicted_labels = [model.predict(np.expand_dims(image/255.0, axis=0))[0][0] for image in images]\n",
    "    predictions = np.array(predicted_labels)\n",
    "    data_frame['label'] = predictions\n",
    "    submission = pd.concat([submission, data_frame[[\"id\", \"label\"]]])\n",
    "submission.to_csv(KAGGLE_SUBMISSION_FILE, index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
